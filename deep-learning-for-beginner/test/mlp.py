# -*- coding: utf-8 -*-
"""딥러닝의 실제와 응용_week04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xdQrYVmuE_oaN93OfAXuEA7ezEFExnGE

# MLP 만들기
"""

# library 
import pandas as pd 
# mnist sample data read 
mnist_csv= pd.read_csv("./sample_data/mnist_train_small.csv", header=None, skiprows=1).values
print(mnist_csv.shape)

mnist_csv
# 2D space -> 1D space로 만듦 -> 784개가 됨
# 첫 번째 행 : 데이터의 밝기를 나타냄 
# 첫 번째 행에 대한 해석 : 레이블이 5인 데이터 한 줄

# sklearn import, tensorflow keras 등장 전에 활용했던 머신러닝 패키지 
from sklearn.model_selection import train_test_split 

# data split 
# 30%가 테스트 데이터, random_state : 랜덤 시드 
train, test = train_test_split(mnist_csv, test_size=0.3, random_state=1)

from tensorflow.keras import utils
Y_train, X_train= utils.to_categorical(train[:,0]), train[:,1:]
Y_test, X_test= utils.to_categorical(test[:,0]), test[:,1:]
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

train[:,0] # 모든 행에 대해 가지고 오는 것

utils.to_categorical(train[:,0])[0]

# 최소 0, 최대 1로 구성된 normalization 
X_train_norm = (X_train -0)/255 #normalize 
X_test_norm =(X_test -0)/255 #normalize 
print(Y_train[0]) 
print(X_train_norm[0])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
plt.imshow(np.reshape(X_train_norm[0], (28, 28)), cmap=plt.cm.Blues)

# Build MLP structure
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,Activation
from tensorflow.keras.optimizers import SGD
mlp_model= Sequential()
mlp_model.add(Dense(512, activation='relu', input_shape=(784,)))
mlp_model.add(Dense(256, activation='relu'))
mlp_model.add(Dense(10, activation='softmax'))
mlp_model.summary()

mlp_model= Sequential()
mlp_model.add(Dense(1024, activation='relu', input_shape=(784,)))
mlp_model.add(Dense(512, activation='relu'))
mlp_model.add(Dense(10, activation='softmax'))
mlp_model.summary()

mlp_model= Sequential()
mlp_model.add(Dense(512, activation='elu', input_shape=(784,)))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dense(10, activation='softmax'))
mlp_model.summary()

# Dropout 
# 의도적으로 파라미터를 제거 
# 장점 : 학습이 되지 않다보니, 파라미터를 줄이는 효과가 있음 
# 단점 : 만능은 아님, 강제적으로 파라미터를 줄이다보니, 전체적인 성능 하락이 발생함
mlp_model= Sequential() 
mlp_model.add(Dense(512, activation='elu', input_shape=(784,)))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dropout(0.2))
mlp_model.add(Dense(10, activation='softmax'))
mlp_model.summary()

# 내가 만든 모델 
mlp_model= Sequential()
mlp_model.add(Dense(512, activation='elu', input_shape=(784,)))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dense(512, activation='elu'))
mlp_model.add(Dropout(0.3))
mlp_model.add(Dense(1024, activation='elu'))
mlp_model.add(Dense(1024, activation='relu'))
mlp_model.add(Dense(256, activation='elu'))
mlp_model.add(Dense(1024, activation='relu'))
mlp_model.add(Dropout(0.3))
mlp_model.add(Dense(10, activation='softmax'))
mlp_model.summary()

# Build loss function and optimizer 
# stocastic gradient descent 
sgd=SGD(lr=0.005) #lr=initiallearningrate 
mlp_model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])

# training and evaluation 
model_history=mlp_model.fit(X_train_norm,
                            Y_train,
                            epochs = 15, # 모델이 얼마나 학습할 지(epochs = 1 -> x train을 한 번 돈다. 15 -> 15번 돈다.)
                            batch_size = 256, # 여러 개의 데이터를 병렬적으로 학습, 얼마나 뽑아서 도는 지, 256개를 한꺼번에 진행
                            verbose = 2, # 출력 함수, 0 = 아무것도 출력 안됨
                            validation_data = (X_test_norm, Y_test), # 학습에는 사용되지 않지만, 모델 적용 시 얼마나 용이한 지 보는 것
                            shuffle = True) # 데이터를 섞을 지 말 지에 대해서 결정하는 것



print(model_history.history.keys()) #[‘acc’, ‘loss’, ‘val_acc’, ‘val_loss’]

plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('modelloss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'],loc='right')
plt.show()

# Plot Accuracy Curve
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='right')
plt.show()

from sklearn.metrics import confusion_matrix
import itertools

def plot_confusion_matrix(model_input, feature, label, class_info):
  pred=model_input.predict(feature)
  cnf_matrix=confusion_matrix(np.argmax(label,axis=1),np.argmax(pred,axis=1))
  plt.figure()
  plt.imshow(cnf_matrix,interpolation='nearest',cmap=plt.cm.Blues)
  tick_marks= np.arange(len(class_info))
  plt.xticks(tick_marks,class_info,rotation=45)
  plt.yticks(tick_marks,class_info)
  thresh=cnf_matrix.max()/2.
  for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
    plt.text(j, i, cnf_matrix[i, j], horizontalalignment="center", color="white" 
             if cnf_matrix[i, j] > thresh else "black")
  plt.tight_layout()
  plt.ylabel('Truelabel')
  plt.xlabel('Predictedlabel')
  plt.show()

label = ['0','1','2','3','4','5','6','7','8','9']
plot_confusion_matrix(mlp_model, X_test, Y_test, class_info=label)







